# -*- coding: utf-8 -*-
"""Model Building-checkpoint.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EIwlPbN18htxyUNQywoxApOW7fRL5f9D
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.python.keras.models import Model
from sklearn.linear_model import LogisticRegression

from google.colab import drive
drive.mount('/content/drive')



data=pd.read_csv('/content/loan-prediction.csv')
data.head()

data.tail()

data.shape

import numpy as num
from sklearn.model_selection import train_test_split
x,y= num.arange(10).reshape((5,2)),range(5)

x

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.3,random_state=5)

"""# Testing Model With Multiple Evaluation Metrics"""

data.info()

# checking for missing values
data.isnull().sum()

# statistical measures about the data
data.describe()

# checking the distribution of Target Variable
data['Credit_History'].value_counts()

X = data.drop(columns='Credit_History', axis=1)
Y = data['Credit_History']

print(X)

print(Y)

import numpy

ar=np.array([1,2,3,4,np.nan,5,np.nan,6,np.inf]).reshape((3,3))
print(ar)

numpy.isnan(ar) # True wherever nan
numpy.isposinf(ar) # True wherever pos.inf
numpy.isneginf(ar) # True wherever neg-inf
numpy.isinf(ar) # True wherever pos-inf or neg-inf
numpy.isfinite(ar) #Trure wherever pos-inf or neg-inf or nan

import numpy as num
from sklearn.model_selection import train_test_split
x,y= num.arange(10).reshape((5,2)),range(5)

x

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.3,random_state=5)

from keras.models import Model

Model.summary

# training the LogisticRegression model with Training data

#Model.fit(x_train, y_train)
Model.fit(x_train, y_train)



model = LogisticRegression(max_iter=1000)

"""# Accuracy Score"""

from sklearn.metrics import accuracy_score

# accuracy on training data
x_train_prediction = Model.predict(x_train)
training_data_accuracy = accuracy_score(y_train, x_train_prediction)
print(training_data_accuracy)

print('Accuracy on Training data : ', round(training_data_accuracy*100, 2), '%')

# accuracy on test data
x_test_prediction = model.predict(x_test)
test_data_accuracy = accuracy_score(y_test, x_test_prediction)
print(test_data_accuracy)

print('Accuracy on Test data : ', round(test_data_accuracy*100, 2), '%')

"""# Precision"""

from sklearn.metrics import precision_score

print("Precision Score : ",precision_score(y_test, y_pred, 
                                           pos_label='positive'
                                           Average='micro'))

# precision for training data predictions
precision_train = precision_score(y_train, x_train_prediction)
print('Training data Precision =', precision_train)







"""# Compare The Model"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import xgboost as xgb

def compareModel(x_train,x_test,y_train,y_test):
    DecisionTreeClassifier(x_train,x_test,y_train,y_test)
    print('_'*100)
    RandomForestClassifier(x_train,x_test,y_train,y_test)
    print('_'*100)
    xgb(x_train,x_test,y_train,y_test)
    print('_'*100)
    KNeighborsClassifier(x_train,x_test,y_train,y_test)
    print('_'*100)

compareModel(x_train,x_test,y_train,y_test)

"""# Comparing Model Accuracy Before & After Applying Hyperparameter Tuning"""

